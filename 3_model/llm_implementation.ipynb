{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook introduces self-attention (scaled-dot product attention) mechanism and LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import display_matrix_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple self-attention mechanism (w/o trainable weights)\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Attentions_simplified.png\" alt=\"\" width=\"1100\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Creating dummy input embeddings.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-input_embeddings.png\" alt=\"\" width=\"250\" height=\"550\">\n",
    "\n",
    "#### First let's create a dummy input embedding with the shape of (6 x 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Input embedding vectors dimensions: 6 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "display_matrix_dimension(inputs, x_name=\"Input embedding vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Attention scores\n",
    "\n",
    "#### Now let's select a second row as a query and calculate its attention scores.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-attention_score.png\" alt=\"\" width=\"800\" height=\"450\">\n",
    "\n",
    "- Attention score is a dot-product between query and all input embedding vectors.\n",
    "- A dot-product is the element-wise multiplication of two vectors and summing the products.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-dot_product_attention_score.png\" alt=\"\" width=\"800\" height=\"325\">\n",
    "\n",
    "The following cell of codes calculate the attention scores of second input embedding, $x^{(2)}$ which is selected as query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores of query: \n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token (2nd row of inputs) is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])  # 1 x 6 placeholder tensor\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(f\"Attention scores of query: \\n{attn_scores_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Attention weights\n",
    "\n",
    "#### Attention weights the normalized attention scores which sum up to 1.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-attention_weights.png\" alt=\"\" width=\"800\" height=\"500\">\n",
    "\n",
    "The code below shows the simple normalization of second input token attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights of query (second input embedding vector): \n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of attention scores: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()  # \n",
    "\n",
    "print(f\"Attention weights of query (second input embedding vector): \\n{attn_weights_2_tmp}\")\n",
    "print(\"Sum of attention scores:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In pratice, softmax function is commonly used to normalize the attention scores to get attention weights for better handling extreme values and gradient properties during training.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-softmax.png\" alt=\"\" width=\"400\" height=\"250\">\n",
    "\n",
    "- In the following code, we use PyTorch implementation of softmax function to calculate attention weights which is highly optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights of query: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights of query:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Context vector\n",
    "\n",
    "### 1.4.1. Context vector w.r.t second input embedding vector\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Context_vector.png\" alt=\"\" width=\"800\" height=\"500\">\n",
    "\n",
    "- We calculate the context vector, $z^{(2)}$ w.r.t second input embedding vector, $x^{(2)}$ by multiplying the embedding input tokens, $x^{(i)}$ with attention weights, $‚ç∫_{(2i)}$ and summing the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector w.r.t query: \n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)  # context vector has same dimension as query\n",
    "\n",
    "# Multiply attention weights and input embedding vectors\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(f\"Context vector w.r.t query: \\n{context_vec_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Context vectors for all input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First calculate attention scores for all input embeddings. \n",
    "<img src=\"./images/Attention_mechansim-All_attention_scores.png\" alt=\"\" width=\"1100\" height=\"400\">\n",
    "\n",
    "- Dot-product between two input embeddings (6 x 3) would return a matrix of attention scores (6 x 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for all input embeddings: \n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      ">>> Attention scores dimensions: 6 rows, 6 columns\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(f\"Attention scores for all input embeddings: \\n{attn_scores}\\n\")\n",
    "display_matrix_dimension(attn_scores, x_name=\"Attention scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, normalize attention scores and calculate attention weights.\n",
    "<img src=\"./images/Attention_mechansim-All_attention_weights.png\" alt=\"\" width=\"650\" height=\"500\">\n",
    "\n",
    "- Utilize PyTorch softmax function for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for all input embeddings: \n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      ">>> Attention weights dimensions: 6 rows, 6 columns\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(f\"Attention weights for all input embeddings: \\n{attn_weights}\\n\")\n",
    "display_matrix_dimension(attn_weights, x_name=\"Attention weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify if adding attention weights along column axis results 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: tensor(1.)\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum(attn_weights[1])  # adding row 2 attention weights along column axis\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))  # adding all rows along column axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, calculate context vectors.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-All_context_vectors.png\" alt=\"\" width=\"800\" height=\"400\">\n",
    "\n",
    "- Context vectors are calculated by multiplying attention weights and input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors for all input embeddings: \n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      ">>> Context vectors dimensions: 6 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(f\"Context vectors for all input embeddings: \\n{all_context_vecs}\")\n",
    "display_matrix_dimension(all_context_vecs, x_name=\"Context vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the dimensions of context vectors and input embedding vectors dimensions are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Self-attention mechanism w/ trainable weights (Scaled dot-product attention)\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Attentions.png\" alt=\"\" width=\"1100\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Introducing self-attention mechanism utilized in original transformer, GPT and other popular LLMs.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Single-head_attention.png\" alt=\"\" width=\"800\" height=\"800\">\n",
    "\n",
    "- We introduce three trainable weight matrices: $W_q$, $W_k$, and $W_v$.\n",
    "  \n",
    "- These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$\n",
    "\n",
    "- Embedding dimensions of the input $x$ and the query vector $q$ can be the same or different, depending on the model's design and specific implementation.\n",
    "- In GPT models, the input $x$ and the query vector $q$ embedding dimensions are usually the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]  # second input element\n",
    "d_in = inputs.shape[1]  # the input embedding size, d=3\n",
    "d_out = 2  # the output embedding size, d=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following code, we initialize the three weight matrices $W_q$, $W_k$, and $W_v$.\n",
    "- Note that we are setting `requires_grad=False` to reduce clutter in the outputs for illustration purposes, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Query Weight matrix dimensions: 3 rows, 2 columns\n",
      ">>> Key Weight matrix dimensions: 3 rows, 2 columns\n",
      ">>> Value Weight matrix dimensions: 3 rows, 2 columns\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Initialize three weight matrices\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)  # dimension: 3 x 2\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)  # dimension: 3 x 2\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)  # dimension: 3 x 2\n",
    "\n",
    "display_matrix_dimension(W_query, x_name=\"Query Weight matrix\")\n",
    "display_matrix_dimension(W_key, x_name=\"Key Weight matrix\")\n",
    "display_matrix_dimension(W_value, x_name=\"Value Weight matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For simplicity, we calculate query, key and value vectors w.r.t to second input embedding vector before we do that for all input embedding vectors.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-q_k_v.png\" alt=\"\" width=\"1100\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector for second input embedding: \n",
      "tensor([0.4306, 1.4551])\n",
      "\n",
      "Key vector for second input embedding: \n",
      "tensor([0.4433, 1.1419])\n",
      "\n",
      "Value vector for second input embedding: \n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query  # _2 because it's w.r.t the 2nd input embedding vector\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(f\"Query vector for second input embedding: \\n{query_2}\\n\") \n",
    "print(f\"Key vector for second input embedding: \\n{key_2}\\n\")\n",
    "print(f\"Value vector for second input embedding: \\n{value_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. First, compute keys and values vectors for all input embeddings\n",
    "\n",
    "- Let's project 6 input tokens of from a 3D to 2D embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Next, we compute attention score of second input embedding vector w.r.t second input embedding vector\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-trainable_attention_score.png\" alt=\"\" width=\"800\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score of second embedding vector:\n",
      " tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]  # key vector for second input embedding\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(\"Attention score of second embedding vector:\\n\", attn_score_22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Next, we compute attention scores of all input embeddings w.r.t second input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T  # All attention scores for given query\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. We then calculate attention weights w.r.t second input embedding vector.\n",
    "\n",
    "- We now scale the attention scores by dividing them by the square root of the embedding dimension, $\\sqrt{d_k}$ (i.e., `d_k**0.5`) and normalize using softmax function to compute attention weights of all input embeddings w.r.t. second input embedding:\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-trainable_attention_weights.png\" alt=\"\" width=\"800\" height=\"550\">\n",
    "\n",
    "- We normalize the attention scores by embedding dimension size is to improve training performance by avoid small gradients.\n",
    "- Scaling by the square root of the embedding dimension is the reason why this is self-attention mechanism is called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]  # w.r.t second input embedding\n",
    "scaled_atten_scores = attn_scores_2 / d_k**0.5  # scaling by square root of embedding dimension\n",
    "attn_weights_2 = torch.softmax(scaled_atten_scores, dim=-1)  # scaled attention weights w.r.t x_2\n",
    "print(attn_weights_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Finally, we calculate context vector of all input embeddings w.r.t second input dimension.\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-trainable_attention_context_vector.png\" alt=\"\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector w.r.t second input dimension: \n",
      " tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector w.r.t second input dimension: \\n\", context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. A compact self-attention class\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Single-head_attention.png\" alt=\"\" width=\"800\" height=\"800\">\n",
    "\n",
    "- We can implement a compact self-attention mechanism w/ trainable weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"Compact representation of scaled-dot product attention.\"\"\"\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can utilize the previous implementation using PyTorch's Linear layers, which are equivalent to a matrix multiplication if we disable the bias units\n",
    "- Another big advantage of using `nn.Linear` over our manual `nn.Parameter(torch.rand(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \"\"\"A more optimized self-attention class\"\"\"\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `SelfAttention_v1` and `SelfAttention_v2` give different outputs because they use different initial weights for the weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Causal attention\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Causal_Attentions.png\" alt=\"Context tokens\" width=\"1100\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Masking future words with causal attention\n",
    "\n",
    "### 3.1.1. Applying causal attention mask\n",
    "\n",
    "- In causal attention, the attention weights above the diagonal are masked, ensuring that for any given input, the LLM is unable to utilize future tokens while calculating the context vectors with the attention weight\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Masked_attention_weights.png\" alt=\"Context tokens\" width=\"1100\" height=\"450\">\n",
    "\n",
    "- Causal self-attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions\n",
    "- In simpler words, this ensures that each next word prediction should only depend on the preceding words\n",
    "- To achieve this, for each given token, we mask out the future tokens (the ones that come after the current token in the input text):\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Modified_causal_attention.png\" alt=\"Context tokens\" width=\"450\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest way to mask out future attention weights is by creating a mask via PyTorch's tril function with elements below the main diagonal (including the diagonal itself) set to 1 and above the main diagonal set to 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Causal attention\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-Masked_single-head_attention.png\" alt=\"Context tokens\" width=\"550\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "        # Just to make sure tensors (buffers) are automatically moved to appropiate device (CPU or GPU) along with model\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
    "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # Mask attention scores\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)  # Dropout\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-head attention\n",
    "\n",
    "<img src=\"./images/Attention_mechansim-MHA_Attentions.png\" alt=\"Context tokens\" width=\"1100\" height=\"250\">\n",
    "\n",
    "### Multi-head attention with stacked attention weights.\n",
    "<img src=\"./images/Attention_mechansim-Two-head_attention.png\" alt=\"Context tokens\" width=\"800\" height=\"650\">\n",
    "\n",
    "### We iteratively create two weight matrices.\n",
    "<img src=\"./images/Attention_mechansim-two-head_attentions.png\" alt=\"Context tokens\" width=\"450\" height=\"450\">\n",
    "\n",
    "- This approach is computationally more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "Context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"Context_vecs.shape:\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone multi-head attention (MHA).\n",
    "<img src=\"./images/Attention_mechansim-two-head_attentions_modification.png\" alt=\"Context tokens\" width=\"800\" height=\"400\">\n",
    "\n",
    "- We create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                    diagonal=1)\n",
    "        )  \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out) 2 x 6 x 2\n",
    "        queries = self.W_query(x)  # 2 x 6 x 2\n",
    "        values = self.W_value(x)  # 2 x 6 x 2\n",
    "        # print(\"Initial values shape: \", values.shape)  # 2 x 6 x 2\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)  # 2 x 6 x 2 x 1\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  # 2 x 6 x 2 x 1\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)  # 2 x 6 x 2 x 1\n",
    "        # print(\"Values shape after splitting heads: \", values.shape)   # 2 x 6 x 2 x 1\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # 2 x 2 x 6 x 1\n",
    "        queries = queries.transpose(1, 2)  # 2 x 2 x 6 x 1\n",
    "        values = values.transpose(1, 2)  # 2 x 2 x 6 x 1\n",
    "        # print(\"Values shape after transposing second and third dimension: \", values.shape)  # 2 x 2 x 6 x 1\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head (tokens x tokens)\n",
    "        # print(f\"Attention score shape: {attn_scores.shape}\")  # 2 x 2 x 6 x 6\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # Scaled normalization\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # print(\"Attention weight shape: \", attn_weights.shape)  # 2 x 2 x 6 x 6\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_tmp = attn_weights @ values  # 2 x 2 x 6 x 1\n",
    "        context_vec = context_tmp.transpose(1, 2)  # 2 x 6 x 2 x 1\n",
    "        # print(\"Context vectors before combining heads: \", context_vec.shape)  # 2 x 6 x 2 x 1\n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # print(\"Context vectors after combining heads: \", context_vec.shape)  # 2 x 6 x 2\n",
    "        context_vec = self.out_proj(context_vec) # optional linear projection commonly use in LLMs\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# print(batch.shape)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementing GPT architecture\n",
    "\n",
    "## 5.1. First create layer normalization class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GPT_2-Layer_normalization.png\" alt=\"Context tokens\" width=\"650\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Next implement GELU activation function.\n",
    "\n",
    "- We implement GELU using the following approximation: $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$ (the original GPT-2 model was also trained with this approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"Gaussian error linear unit.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize difference between GELU and ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Then, implement feed-forward network.\n",
    "\n",
    "<img src=\"./images/GPT_2-Feed_forward.png\" alt=\"Context tokens\" width=\"700\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. After that, implement transformer block with skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Finally, our GPT architecture is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GPT_2-GPT.png\" alt=\"Context tokens\" width=\"700\" height=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Create a dummy input for testing GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Initialize random model weights with dummy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length (max num of input tokens model can handle via Positional embedding)\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of parameters in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of token embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in token embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in token embedding layer: 38,597,376\n"
     ]
    }
   ],
   "source": [
    "token_params = sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of parameters in token embedding layer: {token_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainable parameters in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Token embedding matrix is reused as output matrix (weight typing).\"\"\"\n",
    "total_params_gpt2 =  total_params - token_params\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. A function to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GPT_2-Text_generation.png\" alt=\"Context tokens\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GPT_2-token_prediction.png\" alt=\"Context tokens\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model is not trained not trained yet! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
